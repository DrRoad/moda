{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis on the San Francisco 311 data for 1 Hour time intervals\n",
    "Data can be downloaded from: https://data.sfgov.org/City-Infrastructure/311-Cases/vw6y-z8j6/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## The time window to bucket samples\n",
    "TIME_RANGE = '1H'\n",
    "\n",
    "## File path (original data is ~1GB, this is a reduced version with only categories and dates)\n",
    "#Original file:\n",
    "#DATAPATH = \"SF311_simplified.csv\"\n",
    "\n",
    "#Sample raw data:\n",
    "DATAPATH = \"SF_data/SF-311_simplified.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of data (original data contains additional columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample = pd.read_csv(DATAPATH, nrows=5)\n",
    "raw_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(DATAPATH).drop(columns='Unnamed: 0')\n",
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "raw = raw.rename(columns={'Opened': 'date', 'Category': 'category'})\n",
    "\n",
    "## Turn the raw data into a time series (with date as DatetimeIndex)\n",
    "from moda.dataprep.raw_to_ts import raw_to_ts\n",
    "ts = raw_to_ts(raw,date_format=\"%m/%d/%Y %H:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some general stats\n",
    "\n",
    "print(\"Dataset length: \" + str(len(ts)))\n",
    "print(\"Min date: \" + str(ts.index.get_level_values('date').min()))\n",
    "print(\"Max date: \" + str(ts.index.get_level_values('date').max()))\n",
    "\n",
    "print(\"Total time: {}\".format(ts.index.get_level_values('date').max() - ts.index.get_level_values('date').min()))\n",
    "\n",
    "print(\"Dataset contains {} categories.\".format(len(ts['category'].unique())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we decide on the time interval and aggregate items per time and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moda.dataprep.ts_to_range import ts_to_range\n",
    "ranged_ts = ts_to_range(ts,time_range=TIME_RANGE)\n",
    "ranged_ts.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm using dfply because I like its functional-like syntax. This can also be done with plain pandas.\n",
    "#!pip install dfply\n",
    "from dfply import *\n",
    "\n",
    "## Remove categories with less than 1000 items (in more than 10 years) or that existed less than 100 days\n",
    "min_values = 1000\n",
    "min_days = 100\n",
    "\n",
    "\n",
    "\n",
    "categories = ranged_ts.reset_index() >> group_by(X.category) >> \\\n",
    "    summarise(value = np.sum(X.value),duration_in_dataset = X.date.max()-X.date.min()) >> \\\n",
    "    ungroup() >> \\\n",
    "    mask(X.duration_in_dataset.dt.days > min_days) >> \\\n",
    "    mask(X.value > min_values) >> \\\n",
    "    arrange(X.value,ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Filtered dataset contains {0} categories,\\nafter filtering the small ones that existed less than {1} days or had {2} values of less.\".\n",
    "      format(len(categories),min_days,min_values))\n",
    "\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = categories['category'].values\n",
    "num_categories = len(categories)\n",
    "\n",
    "major_category_threshold=11\n",
    "major_categories = category_names[:major_category_threshold]\n",
    "minor_categories = category_names[major_category_threshold:]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "categories[categories['category'].isin(major_categories)].plot(kind='bar',\n",
    "                                                               x='category',\n",
    "                                                               y='value',\n",
    "                                                               title=\"Top \"+str(major_category_threshold-1)+\" common categories on the SF311 dataset\",\n",
    "                                                               ax=axes[0])\n",
    "categories[categories['category'].isin(minor_categories)].plot(kind='bar',\n",
    "                                                               x='category',\n",
    "                                                               y='value',\n",
    "                                                               title=str(major_category_threshold)+\"th to \"+str(num_categories)+\"th most common categories on the SF311 dataset\",\n",
    "                                                               ax=axes[1])\n",
    "\n",
    "plt.savefig(\"category_values.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in requests per category from year to year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the number of values per category per year\n",
    "categories_yearly = ranged_ts.reset_index() >> mutate(year = X.date.dt.year) >> group_by(X.category,X.year) >> \\\n",
    "    summarise(value_per_year = np.sum(X.value),\n",
    "              duration_in_dataset = X.date.max()-X.date.min()) >>\\\n",
    "    ungroup() >> \\\n",
    "    mask(X.value_per_year > (min_values/12.0)) >> \\\n",
    "    arrange(X.value_per_year,ascending=False)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "major_cats_yearly = categories_yearly[categories_yearly['category'].isin(major_categories)]\n",
    "\n",
    "g = sns.factorplot(x='category', y='value_per_year', hue='year', data=major_cats_yearly, kind='bar', size=4, aspect=4,legend=True)\n",
    "g.set_xticklabels(rotation=90)\n",
    "axes = g.axes.flatten()\n",
    "axes[0].set_title(\"Yearly number of incidents for the top \"+str(major_category_threshold-1)+\" categories\")\n",
    "plt.savefig(\"yearly_values.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_cats_yearly = categories_yearly[categories_yearly['category'].isin(minor_categories)]\n",
    "\n",
    "g = sns.factorplot(x='category', y='value_per_year', hue='year', data=minor_cats_yearly, kind='bar', size=4, aspect=4,legend=True)\n",
    "g.set_xticklabels(rotation=90)\n",
    "axes = g.axes.flatten()\n",
    "axes[0].set_title(\"Yearly number of incidents for the \"+str(major_category_threshold)+\"th to \"+str(num_categories)+\"th categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between categories over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_yearly_pivot = categories_yearly.pivot(\"year\", \"category\", \"value_per_year\")\n",
    "categories_yearly_pivot.head()\n",
    "corr = categories_yearly_pivot.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One category inspection\n",
    "#### Example 1: Noise Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Noise Report\"\n",
    "ranged_ts.loc[pd.IndexSlice[:, category], :].reset_index().plot(kind='line',x='date',y='value',figsize=(24,6),linewidth=0.7, \n",
    "                          title = \"Number of incidents per 1 hour for {}\".format(category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Street and Sidewalk Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '2015-11-01'\n",
    "END = '2018-07-01'\n",
    "category = 'Street and Sidewalk Cleaning'\n",
    "cleaning = ranged_ts.loc[pd.IndexSlice[:, category], :].reset_index()\n",
    "cleaning[(cleaning.date > START) & (cleaning.date<=END)].plot(kind='line',x='date',y='value',figsize=(24,6),linewidth=0.7, \n",
    "                          title = \"Number of incidents per 1 hours for {0} between {1} and {2}\".format(category,START,END))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As comparison, let's look at the same time series with different time ranges (30 minutes, 1 hour and 24 hours), only on two months of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moda.dataprep.ts_to_range import ts_to_range\n",
    "ranged_ts_3H = ts_to_range(ts,time_range='3H',pad_with_zeros=True)\n",
    "ranged_ts_30min = ts_to_range(ts,time_range='30min',pad_with_zeros=True)\n",
    "\n",
    "START = '2015-11-01'\n",
    "END = '2016-01-01'\n",
    "category = 'Street and Sidewalk Cleaning'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1,figsize=(20,12))\n",
    "\n",
    "cleaning_30min = ranged_ts_30min.loc[pd.IndexSlice[:, category], :].reset_index()\n",
    "a1=cleaning_30min[(cleaning_30min.date > START) & (cleaning_30min.date<=END)].plot(kind='line',x='date',y='value',linewidth=0.7, ax=axes[0])\n",
    "\n",
    "cleaning_3H = ranged_ts_3H.loc[pd.IndexSlice[:, category], :].reset_index()\n",
    "a2=cleaning_3H[(cleaning_3H.date > START) & (cleaning_3H.date<=END)].plot(kind='line',x='date',y='value',linewidth=0.7, ax=axes[1])\n",
    "\n",
    "cleaning_1H = ranged_ts.loc[pd.IndexSlice[:, category], :].reset_index()\n",
    "a3=cleaning_1H[(cleaning_1H.date > START) & (cleaning_1H.date<=END)].plot(kind='line',x='date',y='value',linewidth=0.7, ax=axes[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are multiple seasonality factors in this time series. Hourly and weekly patterns are visible on the 30 minute interval time series, and the 3 hours interval time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating different models on the SF 1H data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in order to be able to estimate our models, we use [TagAnomaly](https://github.com/Microsoft/TagAnomaly) to tag the points we think are showing trends in the data. Taganomaly can be found here: https://github.com/Microsoft/TagAnomaly\n",
    "Second, we join the tagged dataset with the time series dataset. Each sample which isn't included in the tagged dataset is assumed to be non-trending (or normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add labeled data\n",
    "labels1H = pd.read_csv('SF_data/SF_1H_anomalies_only.csv',usecols=['date','category','value'])\n",
    "labels1H.date = pd.to_datetime(labels1H.date)\n",
    "\n",
    "labels1H['label'] = 1\n",
    "labels1H.sort_values(by='date').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have labels only for 2018, we'll filter out previous years.\n",
    "ts2018 = ranged_ts[ranged_ts.index.get_level_values(0).year == 2018]\n",
    "ts2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1H = pd.merge(ts2018.reset_index(),labels1H,how='left',on=['date','category'])\n",
    "df1H['label'] = np.where(np.isnan(df1H['value_y']),0,1)\n",
    "df1H = df1H.set_index([pd.DatetimeIndex(df1H['date']),'category'])\n",
    "df1H = df1H.drop(columns = ['date','value_y']).rename(columns = {'value_x':'value'})\n",
    "df1H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moda.evaluators import get_metrics_for_all_categories, get_final_metrics\n",
    "from moda.dataprep import read_data\n",
    "from moda.models import TwitterAnomalyTrendinessDetector, MovingAverageSeasonalTrendinessDetector, \\\n",
    "    STLTrendinessDetector, AzureAnomalyTrendinessDetector\n",
    "\n",
    "\n",
    "def run_model(dataset, freq, min_date='01-01-2018', plot=False, model_name='stl', min_value=10,\n",
    "              min_samples_for_category=100):\n",
    "\n",
    "\n",
    "    if model_name == 'twitter':\n",
    "        model = TwitterAnomalyTrendinessDetector(is_multicategory=True, freq=freq, min_value=min_value, threshold=None,\n",
    "                                                 max_anoms=0.49, seasonality_freq=7)\n",
    "\n",
    "    if model_name == 'ma_seasonal':\n",
    "        model = MovingAverageSeasonalTrendinessDetector(is_multicategory=True, freq=freq, min_value=min_value,\n",
    "                                                        anomaly_type='or',\n",
    "                                                        num_of_std=3)\n",
    "\n",
    "    if model_name == 'stl':\n",
    "        model = STLTrendinessDetector(is_multicategory=True, freq=freq, min_value=min_value,\n",
    "                                      anomaly_type='residual',\n",
    "                                      num_of_std=3, lo_delta=0)\n",
    "\n",
    "    if model_name == 'azure':\n",
    "        dirname = os.path.dirname(__file__)\n",
    "        filename = os.path.join(dirname, 'config/config.json')\n",
    "        subscription_key = get_azure_subscription_key(filename)\n",
    "        model = AzureAnomalyTrendinessDetector(is_multicategory=True, freq=freq, min_value=min_value,\n",
    "                                               subscription_key=subscription_key)\n",
    "    \n",
    "    # There is no fit/predict here. We take the entire time series and can evaluate anomalies on all of it or just the last window(s)\n",
    "    prediction = model.predict(dataset, verbose=False)\n",
    "    raw_metrics = get_metrics_for_all_categories(dataset[['value']], prediction[['prediction']], dataset[['label']],\n",
    "                                                 window_size_for_metrics=5)\n",
    "    metrics = get_final_metrics(raw_metrics)\n",
    "    print(metrics)\n",
    "\n",
    "    ## Plot each category\n",
    "    if plot:\n",
    "        print(\"Plotting...\")\n",
    "        model.plot(labels=dataset['label'],savefig=False)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_stl = run_model(df1H,freq='1H',model_name='stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_category(category_dataset,model_name='stl'):\n",
    "\n",
    "    def ts_subplot(plt, series, label):\n",
    "        plt.plot(series, label=label, linewidth=0.5)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    plt.subplot(421, )\n",
    "    ts_subplot(plt, category_dataset['value'], label='Original')\n",
    "    if 'residual_anomaly' in category_dataset:\n",
    "        plt.subplot(422)\n",
    "        ts_subplot(plt, category_dataset['residual_anomaly'], label='Residual anomaly')\n",
    "    if 'trend' in category_dataset:\n",
    "        plt.subplot(423)\n",
    "        ts_subplot(plt, category_dataset['trend'], label='Trend')\n",
    "    if 'trend_anomaly' in category_dataset:\n",
    "        plt.subplot(424)\n",
    "        ts_subplot(plt, category_dataset['trend_anomaly'], label='Trend anomaly')\n",
    "    if 'seasonality' in category_dataset:\n",
    "        plt.subplot(425)\n",
    "        ts_subplot(plt, category_dataset['seasonality'], label='Seasonality')\n",
    "    \n",
    "    plt.subplot(426)\n",
    "    ts_subplot(plt, category_dataset['prediction'], label='Prediction')\n",
    "    \n",
    "    if 'residual' in category_dataset:\n",
    "        plt.subplot(427)\n",
    "        ts_subplot(plt, category_dataset['residual'], label='Residual')\n",
    "\n",
    "    plt.subplot(428)\n",
    "    ts_subplot(plt, category_dataset['label'], label='Labels')\n",
    "    \n",
    "    category = category_dataset.category[0]\n",
    "    \n",
    "    plt.suptitle(\"{} results for category {}\".format(model_name, category))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graffiti = prediction_stl.loc[pd.IndexSlice[:, 'Graffiti'], :].reset_index(level='category', drop=False)\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plot_one_category(graffiti,model_name='STL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series in this case is relatively noisy. The model was more conservative than the labeler in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sewer = prediction_stl.loc[pd.IndexSlice[:, 'Sewer Issues'], :].reset_index(level='category', drop=False)\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plot_one_category(sewer,model_name='STL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we missed the first peak as we didn't have enough historical data to estimate it. Let's compare this result to a different model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ma = run_model(df1H,freq=TIME_RANGE,model_name='ma_seasonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sewer2 = prediction_ma.loc[pd.IndexSlice[:, 'Sewer Issues'], :].reset_index(level='category', drop=False)\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "plot_one_category(sewer2,model_name='MA seasonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model estimates the trend differently, and found some anomalies on the trend series as well. It too couldn't detect the first peak as it requires some historical data to estimate standard deviation and other statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
